# testTaskTo_renue_fullsearch
Можно учитывая встречаемость, выкрутиться как-то через хэш таблицу.
Но как я понял, данные на вход пойдут небольшие датасеты, значит можно выбрать какаой-ниубдь жирный алгоритм, который будет точнее всего работать.
Мне приглянулся алгоритм TF-IDF. 
Учитывая маленький дадасет, буду прогонять через фильтр чтобы не создавали шумы.
В ходе тестирование оказалось, что алгоритм очень сильно чуствителен к окончаниям -> убираю окончания.
В ходе тестирование оказалось, что по сути TF мне не нужен т.к. в каждом отдельном документе слова уникальны.
В этот раз всё логику записи в файл + время засекать буду в Main().

# Скорость -> пусть m - средняя длина слова, n -  кл-во записей, k - кл-во слов в записи
чтение файла: предобработка + добавление в мэпу O(m * n)
подготовка данных: все слова как вектор O(m * n) + каждой записи свой вектор О(m*n*n*1) + нормализация O(n*k)
поиск: построение вектора O(k*n) + сравнение вектора с другими векторамиO(n*n*k)
итого:
## построение -> О(m*n*n)
## поиск -> О(n*n*k) - по сути если все слова в документах разные.

# Память -> пусть m - средняя длина слова, n -  кл-во записей, k - кл-во слов в записи
List<Report> reports - O(n*const*(m+const))
private final Map<Report, double[]> vectors; - O(n*m + n*k)
Map<String, Integer> vocabulary; - O(n*k + 1)
## O(n*m)
Данная программа отлично подойдёт для маленьких датасетов.